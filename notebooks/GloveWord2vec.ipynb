{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained Glove vectors with Scikit-learn and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained Glove word vectors can be downloaded here:\n",
    "\n",
    "http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the embeddings index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the word vectors is pretty straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "GLOVE_DIR = 'glove/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of the embeddings_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1309    ,  0.011308  ,  0.85140002, -0.44016999, -0.92250001,\n",
       "       -0.15672   ,  0.073091  , -0.10772   , -1.40629995, -0.2111    ,\n",
       "       -0.35148999, -1.31570005,  0.38323   , -0.070724  , -0.3457    ,\n",
       "        0.028638  ,  0.26027   ,  0.25404   , -0.38690999, -0.08535   ,\n",
       "       -0.85211998,  1.89660001, -0.032885  ,  0.24192999,  0.28060001,\n",
       "       -0.35846999,  0.13362999, -0.65781999,  0.89960998,  0.58815002,\n",
       "       -0.59443998,  1.26090002, -0.57046002, -0.14374   ,  0.51617998,\n",
       "       -1.11679995, -0.79299998,  1.04059994, -0.86282998, -0.81149   ,\n",
       "        0.80037999,  0.19261   , -0.25137001, -1.15079999,  0.21164   ,\n",
       "        0.75827998,  0.72103   , -0.72103   ,  0.48985001, -0.18956   ,\n",
       "       -0.45517999, -0.37231001, -0.11526   ,  0.37957999, -0.047891  ,\n",
       "       -1.74249995,  0.10357   ,  0.21019   ,  0.81901997,  1.21640003,\n",
       "       -0.53884   ,  0.58485001, -0.26903999,  0.35712001,  0.30651999,\n",
       "        0.22014999, -0.20929   ,  0.77614999, -0.21447   ,  0.36353999,\n",
       "        0.29148   , -1.4016    ,  0.20649   ,  0.62031001,  0.035288  ,\n",
       "        0.25147   ,  0.51182997,  0.14257   , -0.57406002,  0.08113   ,\n",
       "       -0.19202   ,  0.42793   ,  0.62575001, -0.75566   ,  0.095483  ,\n",
       "       -0.25979   , -0.93413001,  0.20027   ,  0.49090001,  0.64787   ,\n",
       "       -0.24009   ,  0.053559  , -0.42335001,  0.39592001, -0.14788   ,\n",
       "       -0.62224001, -0.87282002,  0.83036   ,  0.79966998,  0.16136   ], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['hockey']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we will use the well known 20 newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train documents: 2257\n",
      "Test documents: 1502\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics','sci.med']\n",
    "\n",
    "train_set = fetch_20newsgroups(subset='train',\n",
    "                           categories=categories,\n",
    "                           remove=('headers', 'footers', 'quotes'),\n",
    "                           shuffle=True, \n",
    "                           random_state=42)\n",
    "\n",
    "test_set = fetch_20newsgroups(subset='test',                          \n",
    "                          categories=categories,\n",
    "                          remove=('headers', 'footers', 'quotes'),\n",
    "                          shuffle=True, \n",
    "                          random_state=42)\n",
    "\n",
    "print(\"Train documents: %d\"%(len(train_set.data)))\n",
    "print(\"Test documents: %d\"%(len(test_set.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the embedding vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a Vectorizer class to be used with our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] for w in texts.split() if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                        for texts in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a pipeline using our vectorizer and the SVM classifier (SVC) with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 75.63%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "text_clf = Pipeline([('vectorizer', EmbeddingVectorizer(embeddings_index)),\n",
    "                     ('clf', SVC(kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "text_clf.fit(train_set.data, train_set.target)\n",
    "predicted = text_clf.predict(test_set.data)\n",
    "\n",
    "print(\"Accuracy on the test set: %.2f%%\" % (np.mean(predicted == test_set.target) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 80.23%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf_vectorizer', TfidfVectorizer()),\n",
    "                     ('clf', SVC(kernel=\"linear\"))\n",
    "])\n",
    "\n",
    "text_clf.fit(train_set.data, train_set.target)\n",
    "predicted = text_clf.predict(test_set.data)\n",
    "\n",
    "print(\"Accuracy on the test set: %.2f%%\" % (np.mean(predicted == test_set.target) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input embeddings and transform labels to binary class matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 100, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = EmbeddingVectorizer(embeddings_index)\n",
    "\n",
    "x_train = vect.transform(train_set.data)\n",
    "x_test = vect.transform(test_set.data)\n",
    "\n",
    "y_train = to_categorical(train_set.target) \n",
    "y_test = to_categorical(test_set.target)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 100, 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 100, 1))\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 98, 64)            256       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 30, 128)           24704     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 28, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 87,108\n",
      "Trainable params: 87,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(100,1)))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(categories), activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop', #adam\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it an check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2257 samples, validate on 1502 samples\n",
      "Epoch 1/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5918 - acc: 0.7378 - val_loss: 0.5635 - val_acc: 0.7500\n",
      "Epoch 2/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5692 - acc: 0.7499 - val_loss: 0.5666 - val_acc: 0.7500\n",
      "Epoch 3/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5684 - acc: 0.7500 - val_loss: 0.5623 - val_acc: 0.7500\n",
      "Epoch 4/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5688 - acc: 0.7500 - val_loss: 0.5625 - val_acc: 0.7500\n",
      "Epoch 5/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5682 - acc: 0.7500 - val_loss: 0.5623 - val_acc: 0.7500\n",
      "Epoch 6/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5673 - acc: 0.7500 - val_loss: 0.5664 - val_acc: 0.7500\n",
      "Epoch 7/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5685 - acc: 0.7500 - val_loss: 0.5624 - val_acc: 0.7500\n",
      "Epoch 8/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5670 - acc: 0.7500 - val_loss: 0.5617 - val_acc: 0.7500\n",
      "Epoch 9/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5671 - acc: 0.7500 - val_loss: 0.5663 - val_acc: 0.7500\n",
      "Epoch 10/10\n",
      "2257/2257 [==============================] - 1s - loss: 0.5678 - acc: 0.7500 - val_loss: 0.5656 - val_acc: 0.7500\n",
      "Wall time: 16.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24008b14fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
